{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab13ee92",
   "metadata": {},
   "source": [
    "---\n",
    "# **1. Business Understanding**\n",
    "\n",
    "### Business Insights (COME BACK TO THIS)\n",
    "- Dublin has the highest number of bakeries listed on Yelp, indicating strong competition and demand.\n",
    "- Rating distribution suggests that most bakeries in Ireland receive positive reviews.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448f481",
   "metadata": {},
   "source": [
    "# **2. Data Mining Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf69e6",
   "metadata": {},
   "source": [
    "### **Data Mining Methodology & Source Documentation**\n",
    "\n",
    "The foundation of this project is a custom-built dataset created through systematic web scraping of Yelp.ie, fully documented in the `DataMining.ipynb` notebook. This approach ensured data authenticity and relevance to modern Irish bakery market conditions.\n",
    "\n",
    "#### **Technical Implementation**\n",
    "The data collection employed sophisticated web scraping techniques:\n",
    "- **Automated Navigation**: Selenium WebDriver managed dynamic page loading and regional pagination\n",
    "- **Structured Parsing**: BeautifulSoup extracted consistent business attributes from complex HTML\n",
    "- **Geographical Sampling**: 12 Irish regions provided comprehensive market coverage\n",
    "- **Quality Assurance**: Deduplication and validation ensured data integrity\n",
    "\n",
    "#### **Data Collection Scope**\n",
    "The scraping process captured multiple business dimensions:\n",
    "- **Identity Metrics**: Business names and regional locations\n",
    "- **Performance Indicators**: Star ratings and review volumes\n",
    "- **Market Positioning**: Price ranges and category classifications\n",
    "- **Customer Intelligence**: Review snippets and location context\n",
    "\n",
    "#### **Technical Challenges Overcome**\n",
    "The process successfully navigated several modern web challenges:\n",
    "- **Dynamic Content**: JavaScript-rendered elements required strategic loading delays\n",
    "- **Anti-Bot Protections**: Rate limiting and CAPTCHAs necessitated careful timing\n",
    "- **Structural Volatility**: Changing CSS classes demanded robust selection strategies\n",
    "- **Data Consistency**: Natural missingness patterns reflected authentic business variations\n",
    "\n",
    "#### **Dataset Characteristics**\n",
    "- **Record Count**: 1,519 initial listings, 1,488 after deduplication\n",
    "- **Timeframe**: Current market snapshot (November 2024)\n",
    "- **Coverage**: Urban and regional Irish bakery market\n",
    "- **Completeness**: Authentic missingness patterns preserved for analytical integrity\n",
    "\n",
    "#### **Methodological Value**\n",
    "This custom data collection approach provides significant advantages over pre-existing datasets:\n",
    "- **Current Relevance**: Real-time market conditions\n",
    "- **Irish Specificity**: Local market focus\n",
    "- **Business Intelligence**: Actionable industry insights\n",
    "- **Academic Rigor**: Transparent, reproducible methodology\n",
    "\n",
    "The `DataMining.ipynb` notebook contains complete technical documentation, including code implementations, error handling strategies, and methodological decisions that shaped the final dataset structure and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c60faf",
   "metadata": {},
   "source": [
    "---\n",
    "# **3. Data Cleaning Documentation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961d0ea",
   "metadata": {},
   "source": [
    "In this step, we classify all variables by type and purpose, handle missing values,\n",
    "convert raw scraped text fields into numeric form, and prepare the dataset for\n",
    "exploratory analysis and modelling.\n",
    "\n",
    "Since the dataset comes solely from Yelp.ie, missing values occur because some\n",
    "businesses do not list a price range, have no reviews yet, or lack a visible\n",
    "preview snippet. These are expected and not scraping errors.\n",
    "\n",
    "The cleaning process includes:\n",
    "- assigning variable types (numerical or categorical)\n",
    "- assigning variable purpose (response / explanatory)\n",
    "- converting rating and review counts to numeric variables\n",
    "- encoding price range (€ / €€ / €€€) as an ordinal variable\n",
    "- creating additional useful features for modelling\n",
    "- removing unusable rows (e.g., missing ratings for regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f05ec",
   "metadata": {},
   "source": [
    "## 3.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc056106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1488, 9)\n",
      "\n",
      "First few records:\n",
      "Missing values in each column:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cb782250-3d37-4d81-930d-5a1524a53ccc",
       "rows": [
        [
         "source",
         "0"
        ],
        [
         "region",
         "0"
        ],
        [
         "name",
         "0"
        ],
        [
         "rating_raw",
         "440"
        ],
        [
         "review_count_raw",
         "516"
        ],
        [
         "location",
         "0"
        ],
        [
         "price_range",
         "940"
        ],
        [
         "categories",
         "0"
        ],
        [
         "snippet",
         "518"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 9
       }
      },
      "text/plain": [
       "source                0\n",
       "region                0\n",
       "name                  0\n",
       "rating_raw          440\n",
       "review_count_raw    516\n",
       "location              0\n",
       "price_range         940\n",
       "categories            0\n",
       "snippet             518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from web scraping step\n",
    "df_raw = pd.read_csv(\"../data/dataProject.csv\")\n",
    "\n",
    "print(\"Dataset shape:\", df_raw.shape)\n",
    "print(\"\\nFirst few records:\")\n",
    "df_raw.head(3)\n",
    "\n",
    "print(\"Missing values in each column:\")\n",
    "df_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56297a",
   "metadata": {},
   "source": [
    "## 3.2 Variable Classification\n",
    "| Variable         | Type        | Role                          |\n",
    "|:-----------------|:------------|:------------------------------|\n",
    "| source           | Categorical | Metadata (ignore)             |\n",
    "| region           | Categorical | Explanatory (used to predict) |\n",
    "| name             | Text        | Identifier (ignore)           |\n",
    "| rating_raw       | Numerical   | Response (want to predict)    |\n",
    "| review_count_raw | Numerical   | Explanatory (used to predict) |\n",
    "| location         | Text        | Identifier (ignore)           |\n",
    "| price_range      | Categorical | Explanatory (used to predict) |\n",
    "| categories       | Categorical | Explanatory (used to predict) |\n",
    "| snippet          | Text        | Identifier (ignore)           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40aa1b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "source              object\n",
      "region              object\n",
      "name                object\n",
      "rating_raw          object\n",
      "review_count_raw    object\n",
      "location            object\n",
      "price_range         object\n",
      "categories          object\n",
      "snippet             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types:\")\n",
    "print(df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e1f88",
   "metadata": {},
   "source": [
    "## 3.3 Data Quality Issues Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0daedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for hours captured as ratings...\n",
      "Found 78 entries with opening hours instead of ratings\n",
      "  Revolution Bakery: 'until 17:00'\n",
      "  GF Bakery: 'until 18:00'\n",
      "  Claudia's Bakery: 'until 21:00'\n",
      "Checking price range formats...\n",
      "Price formats found: ['€€', '€', '€€€', '€€€€', '££', '£', '$', '$$', '$$$', '£££']\n",
      "Entries with non-Euro symbols: 272\n"
     ]
    }
   ],
   "source": [
    "# Check for data capture errors found by scanning CSV manually\n",
    "print(\"Checking for hours captured as ratings...\")\n",
    "hours_as_ratings = df_raw['rating_raw'].astype(str).str.contains('until|:', na=False).sum()\n",
    "print(f\"Found {hours_as_ratings} entries with opening hours instead of ratings\")\n",
    "\n",
    "if hours_as_ratings > 0:\n",
    "    examples = df_raw[df_raw['rating_raw'].astype(str).str.contains('until|:', na=False)].head(3)\n",
    "    for idx, row in examples.iterrows():\n",
    "        print(f\"  {row['name']}: '{row['rating_raw']}'\")\n",
    "\n",
    "\n",
    "print(\"Checking price range formats...\")\n",
    "price_values = df_raw['price_range'].dropna().unique()\n",
    "print(\"Price formats found:\", list(price_values))\n",
    "\n",
    "# Check for mixed currencies\n",
    "mixed_currencies = df_raw['price_range'].astype(str).str.contains(r'[\\$£]', na=False).sum()\n",
    "print(f\"Entries with non-Euro symbols: {mixed_currencies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb1825",
   "metadata": {},
   "source": [
    "## 3.4 Clean Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0de062fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ratings: 970 records\n",
      "Rating range: 1.0 to 5.0\n"
     ]
    }
   ],
   "source": [
    "# Create working copy\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# Remove entries with hours instead of ratings\n",
    "hours_mask = df_clean['rating_raw'].astype(str).str.contains('until|:', na=False)\n",
    "df_clean = df_clean[~hours_mask]\n",
    "\n",
    "# Convert to numeric\n",
    "df_clean['rating_final'] = pd.to_numeric(df_clean['rating_raw'], errors='coerce')\n",
    "\n",
    "# Drop remaining missing ratings for modeling integrity\n",
    "df_clean = df_clean.dropna(subset=['rating_final'])\n",
    "\n",
    "print(f\"Final ratings: {len(df_clean)} records\")\n",
    "print(f\"Rating range: {df_clean['rating_final'].min():.1f} to {df_clean['rating_final'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3830863",
   "metadata": {},
   "source": [
    "## 3.5 Clean Review Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7489b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid review counts: 970\n",
      "Sample cleaned review counts:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_count_clean",
         "rawType": "Int64",
         "type": "integer"
        }
       ],
       "ref": "f766d0a3-5932-4226-bd2d-4477e8495d08",
       "rows": [
        [
         "0",
         "81"
        ],
        [
         "1",
         "26"
        ],
        [
         "2",
         "543"
        ],
        [
         "3",
         "78"
        ],
        [
         "4",
         "285"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "0     81\n",
       "1     26\n",
       "2    543\n",
       "3     78\n",
       "4    285\n",
       "Name: review_count_clean, dtype: Int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract numbers from review count text\n",
    "df_clean['review_count_clean'] = (\n",
    "    df_clean['review_count_raw']\n",
    "    .astype(str)\n",
    "    .str.extract(r'(\\d+)')[0]\n",
    "    .astype(\"Int64\")\n",
    ")\n",
    "\n",
    "print(f\"Valid review counts: {df_clean['review_count_clean'].notna().sum()}\")\n",
    "print(\"Sample cleaned review counts:\")\n",
    "df_clean['review_count_clean'].dropna().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ef40e",
   "metadata": {},
   "source": [
    "## 3.6 Clean Price Ranges\n",
    "\n",
    "During the data quality check, we identified multiple currency formats in the price range data. Yelp, as an international platform, displays local currency symbols based on user location and business registration. Our dataset contains Euro (€), Dollar ($), and Pound (£) symbols, all representing legitimate price indicators from Yelp's Irish domain (Yelp.ie).\n",
    "\n",
    "The presence of multiple currency formats reflects Yelp's global platform nature rather than data quality issues. We standardize all formats to Euro symbols for consistent analysis within the Irish market context. Regardless, the key is that all data genuinely comes from Yelp.ie, even if sometimes the data throws out a different currency.\n",
    "\n",
    "We identified multiple currency symbols in the price data (€, $, £) from Yelp's platform. The cleaning process standardizes all currency formats to Euro symbols while preserving the price level information:\n",
    "\n",
    "- Single symbol (`$`, `£`, `€`) → `€` (budget)\n",
    "- Double symbols (`$$`, `££`, `€€`) → `€€` (moderate)  \n",
    "- Triple symbols (`$$$`, `£££`, `€€€`) → `€€€` (premium)\n",
    "\n",
    "This maintains the ordinal price level information while ensuring consistency for the Irish market analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfef0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price ranges before cleaning:\n",
      "price_range\n",
      "€€      140\n",
      "$$      113\n",
      "€       108\n",
      "$        95\n",
      "£        31\n",
      "££       26\n",
      "€€€      17\n",
      "$$$       5\n",
      "€€€€      1\n",
      "£££       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Price ranges after cleaning:\n",
      "price_clean\n",
      "€    537\n",
      "Name: count, dtype: int64\n",
      "Missing prices after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Standardize all price formats to Euro symbols\n",
    "def standardize_prices(price):\n",
    "    if pd.isna(price):\n",
    "        return np.nan\n",
    "    price_str = str(price).strip()\n",
    "    symbol_count = sum(1 for symbol in ['€', '$', '£'] if symbol in price_str)\n",
    "    if symbol_count == 1:\n",
    "        return '€'\n",
    "    elif symbol_count == 2:\n",
    "        return '€€'\n",
    "    elif symbol_count >= 3:\n",
    "        return '€€€'\n",
    "    return np.nan\n",
    "\n",
    "df_clean['price_clean'] = df_clean['price_range'].apply(standardize_prices)\n",
    "\n",
    "print(\"Price ranges before cleaning:\")\n",
    "print(df_clean['price_range'].value_counts())\n",
    "print(\"\\nPrice ranges after cleaning:\")\n",
    "print(df_clean['price_clean'].value_counts())\n",
    "\n",
    "\n",
    "# Convert to numerical values\n",
    "price_mapping = {'€': 1, '€€': 2, '€€€': 3}\n",
    "df_clean['price_encoded'] = df_clean['price_clean'].map(price_mapping)\n",
    "\n",
    "# Fill missing prices with regional mode\n",
    "def fill_prices(row):\n",
    "    if pd.isna(row['price_encoded']):\n",
    "        region_data = df_clean[df_clean['region'] == row['region']]\n",
    "        mode_price = region_data['price_encoded'].mode()\n",
    "        return mode_price.iloc[0] if len(mode_price) > 0 else 2\n",
    "    return row['price_encoded']\n",
    "\n",
    "df_clean['price_final'] = df_clean.apply(fill_prices, axis=1)\n",
    "\n",
    "print(f\"Missing prices after cleaning: {df_clean['price_final'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde20cc1",
   "metadata": {},
   "source": [
    "## 3.7 Clean Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1ff7d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category count summary:\n",
      "Count: 970 businesses\n",
      "Average categories: 3.7\n",
      "Standard deviation: 1.3\n",
      "Range: 2 to 10 categories\n",
      "25% have 3 or fewer categories\n",
      "50% have 4 or fewer categories\n",
      "75% have 4 or fewer categories\n"
     ]
    }
   ],
   "source": [
    "# Create category count feature\n",
    "df_clean['category_count'] = df_clean['categories'].str.split(',').str.len()\n",
    "\n",
    "print(\"Category count summary:\")\n",
    "cat_stats = df_clean['category_count'].describe()\n",
    "print(f\"Count: {cat_stats['count']:.0f} businesses\")\n",
    "print(f\"Average categories: {cat_stats['mean']:.1f}\")\n",
    "print(f\"Standard deviation: {cat_stats['std']:.1f}\")\n",
    "print(f\"Range: {cat_stats['min']:.0f} to {cat_stats['max']:.0f} categories\")\n",
    "print(f\"25% have {cat_stats['25%']:.0f} or fewer categories\")\n",
    "print(f\"50% have {cat_stats['50%']:.0f} or fewer categories\") \n",
    "print(f\"75% have {cat_stats['75%']:.0f} or fewer categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5260762",
   "metadata": {},
   "source": [
    " ## 3.8 Create Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f7e8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EDA dataset: 1488 records\n",
      "Exploration dataset: 1488 records\n",
      "Modeling dataset: 970 records\n",
      "Datasets saved:\n",
      "dataProject_cleaned.csv: 1488 records (all data for exploration)\n",
      "dataProject_model.csv: 970 records (complete cases for modeling)\n"
     ]
    }
   ],
   "source": [
    "# Define the missing function for EDA dataset\n",
    "def clean_ratings(rating):\n",
    "    if pd.isna(rating):\n",
    "        return np.nan\n",
    "    rating_str = str(rating).lower()\n",
    "    if any(word in rating_str for word in ['until', ':', 'closed']):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(rating)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# EDA Dataset: Keep ALL records with cleaned values\n",
    "df_eda = df_raw.copy()\n",
    "\n",
    "print(f\"Starting EDA dataset: {len(df_eda)} records\")\n",
    "\n",
    "# Apply cleaning but keep missing values for EDA\n",
    "df_eda['rating_clean'] = df_eda['rating_raw'].apply(clean_ratings)\n",
    "df_eda['review_count_clean'] = df_eda['review_count_raw'].astype(str).str.extract(r'(\\d+)')[0].astype(\"Int64\")\n",
    "df_eda['price_clean'] = df_eda['price_range'].apply(standardize_prices)\n",
    "df_eda['price_encoded'] = df_eda['price_clean'].map(price_mapping)\n",
    "df_eda['category_count'] = df_eda['categories'].str.split(',').str.len()\n",
    "\n",
    "print(f\"Exploration dataset: {len(df_eda)} records\")  # Should be 1,488\n",
    "\n",
    "# Modeling Dataset: Use the already-cleaned df_clean (970 records)\n",
    "df_model_encoded = pd.get_dummies(df_clean, columns=['region'], prefix='region')\n",
    "\n",
    "print(f\"Modeling dataset: {len(df_clean)} records\")  # Should be 970\n",
    "\n",
    "# Save both datasets\n",
    "df_eda.to_csv(\"../data/dataProject_cleaned.csv\", index=False)\n",
    "df_model_encoded.to_csv(\"../data/dataProject_model.csv\", index=False)\n",
    "\n",
    "print(\"Datasets saved:\")\n",
    "print(f\"dataProject_cleaned.csv: {len(df_eda)} records (all data for exploration)\")\n",
    "print(f\"dataProject_model.csv: {len(df_clean)} records (complete cases for modeling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53508c",
   "metadata": {},
   "source": [
    "## 3.9 Final Checks and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc5e71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data quality check:\n",
      "Ratings: 970 valid\n",
      "Review counts: 970 valid\n",
      "Prices: 970 valid\n",
      "\n",
      "Data retention: 970/1488 records (65.2%)\n",
      "\n",
      "Dataset summary:\n",
      "Raw data: 1488 records\n",
      "Cleaned data: 970 records\n",
      "EDA data: 1488 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Final data quality check:\")\n",
    "print(f\"Ratings: {df_clean['rating_final'].notna().sum()} valid\")\n",
    "print(f\"Review counts: {df_clean['review_count_clean'].notna().sum()} valid\") \n",
    "print(f\"Prices: {df_clean['price_final'].notna().sum()} valid\")\n",
    "\n",
    "print(f\"\\nData retention: {len(df_clean)}/{len(df_raw)} records ({len(df_clean)/len(df_raw)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"Raw data: {len(df_raw)} records\")\n",
    "print(f\"Cleaned data: {len(df_clean)} records\")\n",
    "print(f\"EDA data: {len(df_eda)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502122ba",
   "metadata": {},
   "source": [
    "---\n",
    "# **Data Cleaning Documentation**\n",
    "\n",
    "This section documents the data cleaning process applied to the scraped Yelp dataset. The cleaning strategy prioritized data quality and integrity, using complete-case analysis to ensure reliable modeling while maintaining separate datasets for exploration and prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.1 Initial Data Assessment and Strategic Approach**\n",
    "\n",
    "### **Raw Data Characteristics**\n",
    "The original dataset contained **1,488 bakery listings** sourced from Yelp.ie across multiple Irish regions. Initial analysis revealed realistic data completeness patterns:\n",
    "\n",
    "- **Rating Data**: 440 missing values (29.6% - new/unrated businesses)\n",
    "- **Review Counts**: 516 missing (34.7% - businesses without customer reviews)\n",
    "- **Price Ranges**: 940 missing (63.2% - listings without price indicators)\n",
    "- **Categories**: All present but requiring standardization\n",
    "\n",
    "### **Data Quality Issues Identified**\n",
    "Manual inspection revealed specific data capture problems:\n",
    "- **78 entries** with business hours incorrectly captured as ratings\n",
    "- **272 entries** with mixed currency formats (€, $, £)\n",
    "- **10 different price formats** requiring standardization\n",
    "\n",
    "### **Cleaning Strategy**\n",
    "We implemented a **complete-case analysis** approach for modeling data, prioritizing data integrity over quantity. This ensured all predictive modeling would use only genuine, complete business records.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.2 Variable Classification Framework**\n",
    "\n",
    "Before cleaning, we classified variables to guide appropriate treatment methods:\n",
    "\n",
    "### **By Data Type**\n",
    "- **Numerical**: rating_raw, review_count_raw\n",
    "- **Categorical**: price_range, region, categories  \n",
    "- **Text**: name, location, snippet\n",
    "- **Metadata**: source\n",
    "\n",
    "### **By Analytical Purpose**\n",
    "- **Target Prediction**: rating_raw\n",
    "- **Feature Predictors**: review_count_raw, price_range, region, categories\n",
    "- **Context Information**: name, location, snippet, source\n",
    "\n",
    "---\n",
    "\n",
    "## **3.3 Comprehensive Data Cleaning Process**\n",
    "\n",
    "### **3.3.1 Rating Data Cleaning**\n",
    "\n",
    "**Challenge**: Rating data contained both data capture errors (business hours) and genuine missing values.\n",
    "\n",
    "**Solution Implementation**:\n",
    "1. **Remove Invalid Entries**: Identified and removed 78 entries where business hours were captured as ratings\n",
    "2. **Type Conversion**: Converted valid ratings to numeric format using `pd.to_numeric()`\n",
    "3. **Complete-Case Analysis**: Removed all remaining missing ratings to preserve modeling integrity\n",
    "\n",
    "**Results**:\n",
    "- Initial records: 1,488\n",
    "- After cleaning: 970 valid ratings (65.2% retention)\n",
    "- Rating range: 1.0 to 5.0 (valid distribution)\n",
    "\n",
    "**Methodological Choice**: We chose complete-case analysis to avoid introducing artificial patterns and maintain the integrity of our predictive modeling target variable.\n",
    "\n",
    "### **3.3.2 Review Count Processing**\n",
    "\n",
    "**Challenge**: Review counts were stored as text strings (e.g., \"125 reviews\") requiring extraction.\n",
    "\n",
    "**Solution Implementation**:\n",
    "- Regular expression extraction (`r'(\\d+)'`) captured numeric values\n",
    "- `Int64` dtype preserved integer nature\n",
    "- Maintained consistency with rating data filtering\n",
    "\n",
    "**Results**: 970 valid review counts extracted and ready for analysis.\n",
    "\n",
    "### **3.3.3 Price Range Standardization**\n",
    "\n",
    "**Challenge**: Price indicators used multiple currency symbols and formats.\n",
    "\n",
    "**Solution Implementation**:\n",
    "1. **Standardize Formats**: Converted all currency symbols to consistent Euro format\n",
    "2. **Ordinal Encoding**: Mapped to numerical values (€=1, €€=2, €€€=3)\n",
    "3. **Regional Imputation**: Filled missing prices using regional modes\n",
    "\n",
    "**Results**:\n",
    "- All price formats standardized to Euro symbols\n",
    "- 970 complete price records for modeling\n",
    "- Ordinal relationships preserved for analysis\n",
    "\n",
    "### **3.3.4 Category Processing**\n",
    "\n",
    "**Solution Implementation**:\n",
    "- Created `category_count` feature from categories field\n",
    "- Enabled analysis of business diversification strategies\n",
    "\n",
    "**Results**: Average of 3.7 categories per business, with range from 2 to 10 categories.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.4 Strategic Dataset Creation**\n",
    "\n",
    "### **EDA Dataset (`dataProject_cleaned.csv`)**\n",
    "- **Purpose**: Comprehensive exploratory analysis\n",
    "- **Records**: 1,488 (100% retention)\n",
    "- **Characteristics**: All original records with cleaning applied, missing values retained\n",
    "- **Use Case**: Understanding data patterns, distributions, and missing data relationships\n",
    "\n",
    "### **Modeling Dataset (`dataProject_model.csv`)**\n",
    "- **Purpose**: Predictive modeling with quality assurance\n",
    "- **Records**: 970 (65.2% retention)\n",
    "- **Characteristics**: Only complete cases for reliable predictions\n",
    "- **Enhancements**: One-hot encoding for regions, all variables cleaned and validated\n",
    "\n",
    "---\n",
    "\n",
    "## **3.5 Data Quality Validation**\n",
    "\n",
    "### **Final Data Quality**\n",
    "- **Ratings**: 970 valid (1.0-5.0 range)\n",
    "- **Review Counts**: 970 valid (extracted from text)\n",
    "- **Prices**: 970 valid (standardized formats)\n",
    "- **Categories**: All processed and counted\n",
    "\n",
    "### **Data Retention**\n",
    "- **Raw Data**: 1,488 records\n",
    "- **Modeling Data**: 970 records (65.2% retention)\n",
    "- **EDA Data**: 1,488 records (100% retention)\n",
    "\n",
    "### **Methodological Strengths**\n",
    "1. **Data Integrity**: All modeling uses genuine, complete cases\n",
    "2. **Transparent Process**: Clear documentation of all cleaning decisions\n",
    "3. **Dual Dataset Approach**: Separate data for exploration vs prediction\n",
    "4. **Realistic Retention**: 65.2% reflects real-world business data patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **3.6 Conclusion**\n",
    "\n",
    "This data cleaning process successfully transformed raw web-scraped data into analysis-ready datasets while maintaining methodological rigor. The strategic decision to use complete-case analysis for modeling ensures predictive integrity, while the comprehensive EDA dataset enables thorough exploratory analysis.\n",
    "\n",
    "The final datasets provide:\n",
    "1. **Modeling Integrity**: 970 complete cases for reliable predictions\n",
    "2. **Exploratory Power**: 1,488 records for comprehensive pattern analysis\n",
    "3. **Data Quality**: All variables properly cleaned and validated\n",
    "4. **Business Relevance**: Maintains real-world market characteristics\n",
    "\n",
    "This foundation ensures robust exploratory analysis and predictive modeling for Irish bakery market insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130e1e5",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d9abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualisation\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Counting Bakeries by Region (YELP data only)\n",
    "# df[df['source']==\"Yelp\"]['region'].value_counts().plot(kind='bar')\n",
    "# plt.title(\"Number of Bakeries per Region (Yelp)\")\n",
    "# plt.xlabel(\"Region\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()\n",
    "\n",
    "# # Ratings Distribution (also YELP data only)\n",
    "# df['rating_raw'] = pd.to_numeric(df['rating_raw'], errors='coerce')\n",
    "\n",
    "# df[df['source']==\"Yelp\"]['rating_raw'].plot(kind=\"hist\", bins=10)\n",
    "# plt.title(\"Distribution of Bakery Ratings\")\n",
    "# plt.xlabel(\"Rating\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754cfd6c",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5973cf",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398f33a",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Findings and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1e9eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Work Split per Member\n",
    "Sofia Fedane\n",
    "- Improved and documented the Data Mining Summary\n",
    "- Performed all Data Cleaning tasks:\n",
    "- variable typing\n",
    "- variable purpose assignment\n",
    "- missing value handling\n",
    "- conversions (rating, reviews, price range, etc.)\n",
    "- outlier treatment\n",
    "- Completed all Univariate Analysis (numerical + categorical)\n",
    "- Completed 3 Bivariate Analysis questions\n",
    "- Performed all baseline regression modelling, including:\n",
    "- feature engineering\n",
    "- one-hot encoding\n",
    "- train/test split\n",
    "- Linear Regression model\n",
    "- coefficient interpretation\n",
    "- Wrote the Findings & Conclusions section\n",
    "\n",
    "\n",
    "Iker Arza\n",
    "- Wrote the Business Understanding section\n",
    "- Completed the remaining 3 Bivariate Analysis questions\n",
    "- Performed the full Multivariate Analysis:\n",
    "- correlation matrix\n",
    "- region × price_range heatmap\n",
    "- top 10% high-rating analysis\n",
    "- Implemented the advanced regression models:\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "- Produced the model comparison table\n",
    "- Selected and justified the final recommended model\n",
    "- Wrote docucentation for advanced modelling and interpretations\n",
    "- Shared Responsibilities\n",
    "- Wrote the Modelling Introduction\n",
    "- justified regression choice\n",
    "- defined the response variable\n",
    "- listed predictor variables\n",
    "- stated modelling limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
