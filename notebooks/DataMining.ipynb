{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb29598",
   "metadata": {},
   "source": [
    "# Data Mining Introduction\n",
    "### Team: Iker Arza & Sofia Fedane\n",
    "\n",
    "**Context:**\n",
    "This project aims to build a real-world dataset of bakeries in Ireland using web scraping techniques taught in class. The goal is to collect business and customer-facing information from multiple online platforms, consolidate it into a single dataset, clean it, and prepare it for analysis and modelling.\n",
    "\n",
    "**Dataset:**\n",
    "Two public online platforms were used:\n",
    "1. GoldenPages.ie: provides contact information such as name, address, phone number, business categories, and short summaries.\n",
    "2. Yelp.ie: provides customer-driven information such as ratings, review counts, price ranges, location tags, and customer review snippets.\n",
    "\n",
    "**Business Motivation**\n",
    "The bakery sector in Ireland is diverse, ranging from small artisan bakeries to large commercial chains. Understanding what attributes make bakeries successful, such as: ratings, category labels, pricing range, location, and customer reviews, may help identify trends related to consumer preferences and regional differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eab327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = 1500\n",
    "\n",
    "GOLDENPAGES_URLS = {\n",
    "    \"bakery\":       \"https://www.goldenpages.ie/q/business/advanced/what/bakery/\",\n",
    "    \"cake_shop\":    \"https://www.goldenpages.ie/q/business/advanced/what/cake%20shop/\",\n",
    "    \"coffee_shop\":  \"https://www.goldenpages.ie/q/business/advanced/what/coffee%20shop/\",\n",
    "    \"dessert_shop\": \"https://www.goldenpages.ie/q/business/advanced/what/dessert%20shop/\",\n",
    "    \"pastry_shop\":  \"https://www.goldenpages.ie/q/business/advanced/what/pastry%20shop/\",\n",
    "}\n",
    "\n",
    "YELP_URLS = {\n",
    "    \"Dublin\":    \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Dublin\",\n",
    "    \"Cork\":      \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Cork\",\n",
    "    \"Galway\":    \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Galway\",\n",
    "    \"Limerick\":  \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Limerick\",\n",
    "    \"Waterford\": \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Waterford\",\n",
    "    \"Kerry\":     \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Kerry\",\n",
    "    \"Louth\":     \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Louth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40b735",
   "metadata": {},
   "source": [
    "## Data Source 1\n",
    "**Data Source: GoldenPages**\n",
    "\n",
    "We scrape business listings from GoldenPages.ie, focusing on bakery-related search terms such as bakery, cake shop, coffee shop, dessert shop, and pastry shop.\n",
    "GoldenPages provides structured business-oriented information, including:\n",
    "- Business name\n",
    "- Physical address\n",
    "- Phone number\n",
    "- Business category\n",
    "- Short business summary / description\n",
    "\n",
    "For each search term, GoldenPages displays listings across multiple pages using numeric pagination (/1, /2, /3, …).\n",
    "\n",
    "In this part of the project, we:\n",
    "- Loop through each bakery-related search term.\n",
    "- Paginate automatically through all available result pages.\n",
    "- Scroll the page to load dynamically-rendered content (using Selenium).\n",
    "- Parse each listing card using BeautifulSoup.\n",
    "- Extract key information fields such as name, address, phone number, category, and summary.\n",
    "- Stop scraping GoldenPages when we reach the global project limit of 1,500 rows or run out of pages.\n",
    "- These rows form the first half of our combined bakery dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727dc7e",
   "metadata": {},
   "source": [
    "## GoldenPages Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18750391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_goldenpages():\n",
    "    rows = []\n",
    "\n",
    "    for search_label, base_url in GOLDENPAGES_URLS.items():\n",
    "        print(f\"\\nGoldenPages search: {search_label}\")\n",
    "\n",
    "        page_number = 1\n",
    "        last_url = None\n",
    "\n",
    "        while True:\n",
    "            if len(rows) >= MAX_ROWS:\n",
    "                print(\"Reached MAX_ROWS in GoldenPages:\", len(rows))\n",
    "                return rows\n",
    "\n",
    "            # Build the URL of the next page\n",
    "            if page_number == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = base_url.rstrip(\"/\") + f\"/{page_number}\"\n",
    "\n",
    "            print(f\"  Page {page_number}: {page_url}\")\n",
    "            driver.get(page_url)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            current_url = driver.current_url\n",
    "\n",
    "            # Stop if the site loops back to the same page\n",
    "            if last_url is not None and current_url == last_url:\n",
    "                print(\"  Same URL encountered; stopping this search term.\")\n",
    "                break\n",
    "            last_url = current_url\n",
    "\n",
    "            # Scroll to load dynamic content\n",
    "            try:\n",
    "                body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                for _ in range(3):\n",
    "                    body.send_keys(Keys.END)\n",
    "                    time.sleep(1)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            cards = soup.find_all(\"div\", class_=\"listing_container\")\n",
    "            print(\"    Cards found:\", len(cards))\n",
    "\n",
    "            if len(cards) == 0:\n",
    "                print(\"  No listings on this page; stopping this search term.\")\n",
    "                break\n",
    "\n",
    "            # Extract the details\n",
    "            for c in cards:\n",
    "                if len(rows) >= MAX_ROWS:\n",
    "                    print(\"Reached MAX_ROWS while processing cards.\")\n",
    "                    return rows\n",
    "\n",
    "                name_tag = c.find(\"a\", class_=\"listing_title_link\")\n",
    "                name = name_tag.get_text(\" \", strip=True) if name_tag else None\n",
    "\n",
    "                addr_tag = c.find(\"div\", class_=\"listing_address\")\n",
    "                address = addr_tag.get_text(\" \", strip=True) if addr_tag else None\n",
    "\n",
    "                phone_tag = c.find(\"a\", class_=\"link_listing_number\")\n",
    "                phone = phone_tag.get_text(strip=True) if phone_tag else None\n",
    "\n",
    "                category_from_page = None\n",
    "                cat_div = c.find(\"div\", class_=\"listing_categories\")\n",
    "                if cat_div:\n",
    "                    li = cat_div.find(\"li\")\n",
    "                    if li:\n",
    "                        category_from_page = li.get_text(\" \", strip=True)\n",
    "\n",
    "                summary = None\n",
    "                summary_div = c.find(\"div\", class_=\"listing_summary\")\n",
    "                if summary_div:\n",
    "                    p = summary_div.find(\"p\")\n",
    "                    if p:\n",
    "                        summary = p.get_text(\" \", strip=True)\n",
    "\n",
    "                rows.append({\n",
    "                    \"source\": \"GoldenPages\",\n",
    "                    \"category_search\": search_label,\n",
    "                    \"name\": name,\n",
    "                    \"address\": address,\n",
    "                    \"phone\": phone,\n",
    "                    \"category_from_page\": category_from_page,\n",
    "                    \"summary\": summary,\n",
    "                })\n",
    "\n",
    "            print(\"    Total collected:\", len(rows))\n",
    "            page_number += 1\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53aaea0",
   "metadata": {},
   "source": [
    "# Data Source 2\n",
    "**Data Source: Yelp**\n",
    "\n",
    "We scrape bakery listings from Yelp.ie across multiple Irish regions, including Dublin, Cork, Galway, Limerick, Waterford, Kerry, and Louth. Yelp provides rich customer-oriented information that complements GoldenPages.\n",
    "\n",
    "The available fields include:\n",
    "- Business name\n",
    "- Star rating\n",
    "- Number of reviews\n",
    "- Price range (€, €€, €€€)\n",
    "- Location / area tags\n",
    "- Business categories (e.g., “Bakery”, “Café”, “Patisserie”)\n",
    "- Short review snippet visible in search results\n",
    "\n",
    "\n",
    "In this part of the project, we:\n",
    "- Loop through each selected Irish region.\n",
    "- Load the search results page for bakeries.\n",
    "- Scroll to dynamically load all visible listings.\n",
    "- Parse each listing card using BeautifulSoup.\n",
    "- Extract key customer-focused features such as rating, review count, price range, categories, and review snippet.\n",
    "- Click the “Next” button until no further pages are available or until the combined total dataset reaches 1,500 rows.\n",
    "\n",
    "These rows form the second half of the dataset and complement the GoldenPages business information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd2749",
   "metadata": {},
   "source": [
    "## Yelp Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yelp(max_rows_remaining, max_pages_per_region=10):\n",
    "    rows = []\n",
    "\n",
    "    for region_label, base_url in YELP_URLS.items():\n",
    "        print(f\"\\nYelp region: {region_label}\")\n",
    "\n",
    "        page_number = 0\n",
    "\n",
    "        while page_number < max_pages_per_region:\n",
    "\n",
    "            if len(rows) >= max_rows_remaining:\n",
    "                print(\"Reached Yelp limit:\", len(rows))\n",
    "                return rows\n",
    "\n",
    "            # Build the page URL\n",
    "            if page_number == 0:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = base_url + f\"&start={page_number * 10}\"\n",
    "\n",
    "            print(f\"  Page {page_number + 1}: {page_url}\")\n",
    "            driver.get(page_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Scroll to ensure full content loads\n",
    "            try:\n",
    "                body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                for _ in range(3):\n",
    "                    body.send_keys(Keys.END)\n",
    "                    time.sleep(1.5)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            cards = soup.find_all(\"div\", attrs={\"data-testid\": \"serp-ia-card\"})\n",
    "\n",
    "            if not cards:\n",
    "                print(\"  No further results for this region.\")\n",
    "                break\n",
    "\n",
    "            # Extract fields from each card\n",
    "            for card in cards:\n",
    "                if len(rows) >= max_rows_remaining:\n",
    "                    print(\"Reached remaining limit inside card loop.\")\n",
    "                    return rows\n",
    "\n",
    "                name_tag = card.find(\"a\", class_=\"y-css-1x1e1r2\")\n",
    "                name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "                rating_tag = card.find(\"span\", class_=\"y-css-f73en8\")\n",
    "                rating_raw = rating_tag.get_text(strip=True) if rating_tag else None\n",
    "\n",
    "                reviews_tag = card.find(\"span\", class_=\"y-css-1vi7y4e\")\n",
    "                review_count_raw = reviews_tag.get_text(strip=True) if reviews_tag else None\n",
    "\n",
    "                loc_tag = card.find(\"span\", class_=\"y-css-wpsy4m\")\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else None\n",
    "\n",
    "                price_tag = card.find(\"span\", class_=\"y-css-1y784sg\")\n",
    "                price_range = price_tag.get_text(strip=True) if price_tag else None\n",
    "\n",
    "                categories = None\n",
    "                cat_container = card.find(\"div\", attrs={\"data-testid\": \"serp-ia-categories\"})\n",
    "                if cat_container:\n",
    "                    categories = \", \".join(\n",
    "                        p.get_text(strip=True) for p in cat_container.find_all(\"p\")\n",
    "                    )\n",
    "\n",
    "                snippet_tag = card.find(\"p\", class_=\"y-css-oyr8zn\")\n",
    "                snippet = snippet_tag.get_text(\" \", strip=True) if snippet_tag else None\n",
    "\n",
    "                rows.append({\n",
    "                    \"source\": \"Yelp\",\n",
    "                    \"region\": region_label,\n",
    "                    \"name\": name,\n",
    "                    \"rating_raw\": rating_raw,\n",
    "                    \"review_count_raw\": review_count_raw,\n",
    "                    \"location\": location,\n",
    "                    \"price_range\": price_range,\n",
    "                    \"categories\": categories,\n",
    "                    \"snippet\": snippet,\n",
    "                })\n",
    "\n",
    "            print(\"    Total collected:\", len(rows))\n",
    "            page_number += 1\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GoldenPages data\n",
    "goldenpages_rows = scrape_goldenpages()\n",
    "current_total = len(goldenpages_rows)\n",
    "print(\"\\nGoldenPages collected:\", current_total, \"rows\")\n",
    "\n",
    "# Collect Yelp data (if needed to reach 1500 rows)\n",
    "if current_total < MAX_ROWS:\n",
    "    remaining = MAX_ROWS - current_total\n",
    "    print(\"Additional rows required:\", remaining)\n",
    "    yelp_rows = scrape_yelp(max_rows_remaining=remaining)\n",
    "else:\n",
    "    print(\"Reached MAX_ROWS from GoldenPages; skipping Yelp\")\n",
    "    yelp_rows = []\n",
    "\n",
    "# Combine sources\n",
    "all_rows = goldenpages_rows + yelp_rows\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "print(\"\\nRows before duplicate removal:\", len(df))\n",
    "\n",
    "# No duplicates (based on name and address\n",
    "df = df.drop_duplicates(subset=[\"name\", \"address\"], keep=\"first\")\n",
    "\n",
    "print(\"Rows after duplicate removal:\", len(df))\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"../data/dataProject.csv\", index=False)\n",
    "print(\"\\nDataset saved to dataProject.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326158b",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining Summary, Issues & Limitations\n",
    "\n",
    "### Overview  \n",
    "The data mining phase combined data from **two independent online sources** (GoldenPages.ie and Yelp.ie). Using Selenium and BeautifulSoup, we extracted bakery-related business information and consolidated it into a single dataset.\n",
    "\n",
    "The goal was a maximum of 1,500 rows; however, after cleaning and deduplication, the final dataset contained 1,091 unique bakery entries, which is sufficient for the later modelling stages.\n",
    "\n",
    "---\n",
    "\n",
    "## What Worked Well\n",
    "\n",
    "### **GoldenPages Scraping**\n",
    "- Pagination across multiple pages (`/1`, `/2`, `/3`, ...).\n",
    "- Selenium scrolled content to load dynamically rendered listings.\n",
    "- The scraper extracted:\n",
    "  - business name\n",
    "  - address\n",
    "  - phone number\n",
    "  - business category\n",
    "  - summary text\n",
    "- Approximately 1,070 listings were gathered from GoldenPages.\n",
    "\n",
    "### **Yelp Scraping**\n",
    "- The scraper navigated through search result pages for 7 counties in Ireland.\n",
    "- Extracted customer-related fields: rating, review count, price range, categories, and review snippet.\n",
    "- The structure of Yelp’s HTML was parsed using `data-testid` attributes.\n",
    "- Pagination using the `start=` parameter worked consistently.\n",
    "\n",
    "---\n",
    "\n",
    "## Challenges & Limitations Encountered\n",
    "\n",
    "### **1. Dynamic Content Loading**\n",
    "Both GoldenPages and Yelp use scripts to load some elements dynamically.\n",
    "Scrolling using Selenium was necessary, as the page showed incomplete data without it.\n",
    "\n",
    "### **2. Pagination Differences**\n",
    "GoldenPages uses a numeric `/page` structure, whereas Yelp uses `start=` indexing.\n",
    "Each required different handling logic.\n",
    "\n",
    "### **3. Duplicate Business Names**\n",
    "Several businesses appeared under multiple search terms (e.g., “bakery”, “cake shop”, “pastry shop”).\n",
    "Deduplication reduced the dataset to 1,091 rows.\n",
    "\n",
    "### **4. Missing Fields**\n",
    "Because the two platforms provide different types of information, the dataset contains structural missing values:\n",
    "- Yelp listings lack phone numbers and business descriptions.\n",
    "- GoldenPages listings lack ratings, reviews, categories, and price ranges.\n",
    "\n",
    "This is expected and not an error; it is expected in multi-source scraping.\n",
    "\n",
    "### **5. Layout Changes During Scraping**\n",
    "Yelp occasionally changed its CSS class names (e.g., dynamic `y-css-*` classes).\n",
    "This required relying on more stable attributes (`data-testid`), which improved reliability.\n",
    "\n",
    "### **6. Regional Limitations**\n",
    "Some counties (Louth, Kerry) had very small numbers of available listings on Yelp, contributing to the lower-than-expected total dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Output\n",
    "\n",
    "- Total records extracted before cleaning: **~1,500**\n",
    "- Total records after deduplication: **1,091**\n",
    "- Dataset saved to: `../data/dataProject.csv`\n",
    "- Columns collected: **14**\n",
    "- Data sources used: **GoldenPages.ie + Yelp.ie**\n",
    "- All scraping completed using:\n",
    "  - `Selenium`\n",
    "  - `BeautifulSoup`\n",
    "  - `pandas`\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
