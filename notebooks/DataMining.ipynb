{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb29598",
   "metadata": {},
   "source": [
    "# Data Mining Introduction\n",
    "### Team: Iker Arza & Sofia Fedane\n",
    "\n",
    "**Context:**\n",
    "This project aims to build a real-world dataset of bakeries in Ireland using web scraping techniques taught in class. The goal is to collect business and customer-facing information from multiple online platforms, consolidate it into a single dataset, clean it, and prepare it for analysis and modelling.\n",
    "\n",
    "**Dataset:**\n",
    "Two public online platforms were used:\n",
    "1. GoldenPages.ie: provides contact information such as name, address, phone number, business categories, and short summaries.\n",
    "2. Yelp.ie: provides customer-driven information such as ratings, review counts, price ranges, location tags, and customer review snippets.\n",
    "\n",
    "**Business Motivation**\n",
    "The bakery sector in Ireland is diverse, ranging from small artisan bakeries to large commercial chains. Understanding what attributes make bakeries successful, such as: ratings, category labels, pricing range, location, and customer reviews, may help identify trends related to consumer preferences and regional differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eab327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = 1500\n",
    "\n",
    "GOLDENPAGES_URLS = {\n",
    "    \"bakery\":       \"https://www.goldenpages.ie/q/business/advanced/what/bakery/\",\n",
    "    \"cake_shop\":    \"https://www.goldenpages.ie/q/business/advanced/what/cake%20shop/\",\n",
    "    \"coffee_shop\":  \"https://www.goldenpages.ie/q/business/advanced/what/coffee%20shop/\",\n",
    "    \"dessert_shop\": \"https://www.goldenpages.ie/q/business/advanced/what/dessert%20shop/\",\n",
    "    \"pastry_shop\":  \"https://www.goldenpages.ie/q/business/advanced/what/pastry%20shop/\",\n",
    "}\n",
    "\n",
    "YELP_URLS = {\n",
    "    \"Dublin\":    \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Dublin\",\n",
    "    \"Cork\":      \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Cork\",\n",
    "    \"Galway\":    \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Galway\",\n",
    "    \"Limerick\":  \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Limerick\",\n",
    "    \"Waterford\": \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Waterford\",\n",
    "    \"Kerry\":     \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Kerry\",\n",
    "    \"Louth\":     \"https://www.yelp.ie/search?find_desc=Bakeries&find_loc=Louth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40b735",
   "metadata": {},
   "source": [
    "## Data Source 1\n",
    "**Data Source: GoldenPages**\n",
    "\n",
    "We scrape business listings from GoldenPages.ie, focusing on bakery-related search terms such as bakery, cake shop, coffee shop, dessert shop, and pastry shop.\n",
    "GoldenPages provides structured business-oriented information, including:\n",
    "- Business name\n",
    "- Physical address\n",
    "- Phone number\n",
    "- Business category\n",
    "- Short business summary / description\n",
    "\n",
    "For each search term, GoldenPages displays listings across multiple pages using numeric pagination (/1, /2, /3, …).\n",
    "\n",
    "In this part of the project, we:\n",
    "- Loop through each bakery-related search term.\n",
    "- Paginate automatically through all available result pages.\n",
    "- Scroll the page to load dynamically-rendered content (using Selenium).\n",
    "- Parse each listing card using BeautifulSoup.\n",
    "- Extract key information fields such as name, address, phone number, category, and summary.\n",
    "- Stop scraping GoldenPages when we reach the global project limit of 1,500 rows or run out of pages.\n",
    "- These rows form the first half of our combined bakery dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727dc7e",
   "metadata": {},
   "source": [
    "## GoldenPages Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18750391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_goldenpages():\n",
    "    rows = []\n",
    "\n",
    "    for search_label, base_url in GOLDENPAGES_URLS.items():\n",
    "        print(f\"\\nGoldenPages search: {search_label}\")\n",
    "\n",
    "        page_number = 1\n",
    "        last_url = None\n",
    "\n",
    "        while True:\n",
    "            if len(rows) >= MAX_ROWS:\n",
    "                print(\"Reached MAX_ROWS in GoldenPages:\", len(rows))\n",
    "                return rows\n",
    "\n",
    "            # Build the URL of the next page\n",
    "            if page_number == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = base_url.rstrip(\"/\") + f\"/{page_number}\"\n",
    "\n",
    "            print(f\"  Page {page_number}: {page_url}\")\n",
    "            driver.get(page_url)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            current_url = driver.current_url\n",
    "\n",
    "            # Stop if the site loops back to the same page\n",
    "            if last_url is not None and current_url == last_url:\n",
    "                print(\"  Same URL encountered; stopping this search term.\")\n",
    "                break\n",
    "            last_url = current_url\n",
    "\n",
    "            # Scroll to load dynamic content\n",
    "            try:\n",
    "                body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                for _ in range(3):\n",
    "                    body.send_keys(Keys.END)\n",
    "                    time.sleep(1)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            cards = soup.find_all(\"div\", class_=\"listing_container\")\n",
    "            print(\"    Cards found:\", len(cards))\n",
    "\n",
    "            if len(cards) == 0:\n",
    "                print(\"  No listings on this page; stopping this search term.\")\n",
    "                break\n",
    "\n",
    "            # Extract the details\n",
    "            for c in cards:\n",
    "                if len(rows) >= MAX_ROWS:\n",
    "                    print(\"Reached MAX_ROWS while processing cards.\")\n",
    "                    return rows\n",
    "\n",
    "                name_tag = c.find(\"a\", class_=\"listing_title_link\")\n",
    "                name = name_tag.get_text(\" \", strip=True) if name_tag else None\n",
    "\n",
    "                addr_tag = c.find(\"div\", class_=\"listing_address\")\n",
    "                address = addr_tag.get_text(\" \", strip=True) if addr_tag else None\n",
    "\n",
    "                phone_tag = c.find(\"a\", class_=\"link_listing_number\")\n",
    "                phone = phone_tag.get_text(strip=True) if phone_tag else None\n",
    "\n",
    "                category_from_page = None\n",
    "                cat_div = c.find(\"div\", class_=\"listing_categories\")\n",
    "                if cat_div:\n",
    "                    li = cat_div.find(\"li\")\n",
    "                    if li:\n",
    "                        category_from_page = li.get_text(\" \", strip=True)\n",
    "\n",
    "                summary = None\n",
    "                summary_div = c.find(\"div\", class_=\"listing_summary\")\n",
    "                if summary_div:\n",
    "                    p = summary_div.find(\"p\")\n",
    "                    if p:\n",
    "                        summary = p.get_text(\" \", strip=True)\n",
    "\n",
    "                rows.append({\n",
    "                    \"source\": \"GoldenPages\",\n",
    "                    \"category_search\": search_label,\n",
    "                    \"name\": name,\n",
    "                    \"address\": address,\n",
    "                    \"phone\": phone,\n",
    "                    \"category_from_page\": category_from_page,\n",
    "                    \"summary\": summary,\n",
    "                })\n",
    "\n",
    "            print(\"    Total collected:\", len(rows))\n",
    "            page_number += 1\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53aaea0",
   "metadata": {},
   "source": [
    "# Data Source 2\n",
    "**Data Source: Yelp**\n",
    "\n",
    "We scrape bakery listings from Yelp.ie across multiple Irish regions, including Dublin, Cork, Galway, Limerick, Waterford, Kerry, and Louth. Yelp provides rich customer-oriented information that complements GoldenPages.\n",
    "\n",
    "The available fields include:\n",
    "- Business name\n",
    "- Star rating\n",
    "- Number of reviews\n",
    "- Price range (€, €€, €€€)\n",
    "- Location / area tags\n",
    "- Business categories (e.g., “Bakery”, “Café”, “Patisserie”)\n",
    "- Short review snippet visible in search results\n",
    "\n",
    "\n",
    "In this part of the project, we:\n",
    "- Loop through each selected Irish region.\n",
    "- Load the search results page for bakeries.\n",
    "- Scroll to dynamically load all visible listings.\n",
    "- Parse each listing card using BeautifulSoup.\n",
    "- Extract key customer-focused features such as rating, review count, price range, categories, and review snippet.\n",
    "- Click the “Next” button until no further pages are available or until the combined total dataset reaches 1,500 rows.\n",
    "\n",
    "These rows form the second half of the dataset and complement the GoldenPages business information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd2749",
   "metadata": {},
   "source": [
    "## Yelp Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yelp(max_rows_remaining, max_pages_per_region=10):\n",
    "    rows = []\n",
    "\n",
    "    for region_label, base_url in YELP_URLS.items():\n",
    "        print(f\"\\nYelp region: {region_label}\")\n",
    "\n",
    "        page_number = 0\n",
    "\n",
    "        while page_number < max_pages_per_region:\n",
    "\n",
    "            if len(rows) >= max_rows_remaining:\n",
    "                print(\"Reached Yelp limit:\", len(rows))\n",
    "                return rows\n",
    "\n",
    "            # Build the page URL\n",
    "            if page_number == 0:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = base_url + f\"&start={page_number * 10}\"\n",
    "\n",
    "            print(f\"  Page {page_number + 1}: {page_url}\")\n",
    "            driver.get(page_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Scroll to ensure full content loads\n",
    "            try:\n",
    "                body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                for _ in range(3):\n",
    "                    body.send_keys(Keys.END)\n",
    "                    time.sleep(1.5)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            cards = soup.find_all(\"div\", attrs={\"data-testid\": \"serp-ia-card\"})\n",
    "\n",
    "            if not cards:\n",
    "                print(\"  No further results for this region.\")\n",
    "                break\n",
    "\n",
    "            # Extract fields from each card\n",
    "            for card in cards:\n",
    "                if len(rows) >= max_rows_remaining:\n",
    "                    print(\"Reached remaining limit inside card loop.\")\n",
    "                    return rows\n",
    "\n",
    "                name_tag = card.find(\"a\", class_=\"y-css-1x1e1r2\")\n",
    "                name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "                rating_tag = card.find(\"span\", class_=\"y-css-f73en8\")\n",
    "                rating_raw = rating_tag.get_text(strip=True) if rating_tag else None\n",
    "\n",
    "                reviews_tag = card.find(\"span\", class_=\"y-css-1vi7y4e\")\n",
    "                review_count_raw = reviews_tag.get_text(strip=True) if reviews_tag else None\n",
    "\n",
    "                loc_tag = card.find(\"span\", class_=\"y-css-wpsy4m\")\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else None\n",
    "\n",
    "                price_tag = card.find(\"span\", class_=\"y-css-1y784sg\")\n",
    "                price_range = price_tag.get_text(strip=True) if price_tag else None\n",
    "\n",
    "                categories = None\n",
    "                cat_container = card.find(\"div\", attrs={\"data-testid\": \"serp-ia-categories\"})\n",
    "                if cat_container:\n",
    "                    categories = \", \".join(\n",
    "                        p.get_text(strip=True) for p in cat_container.find_all(\"p\")\n",
    "                    )\n",
    "\n",
    "                snippet_tag = card.find(\"p\", class_=\"y-css-oyr8zn\")\n",
    "                snippet = snippet_tag.get_text(\" \", strip=True) if snippet_tag else None\n",
    "\n",
    "                rows.append({\n",
    "                    \"source\": \"Yelp\",\n",
    "                    \"region\": region_label,\n",
    "                    \"name\": name,\n",
    "                    \"rating_raw\": rating_raw,\n",
    "                    \"review_count_raw\": review_count_raw,\n",
    "                    \"location\": location,\n",
    "                    \"price_range\": price_range,\n",
    "                    \"categories\": categories,\n",
    "                    \"snippet\": snippet,\n",
    "                })\n",
    "\n",
    "            print(\"    Total collected:\", len(rows))\n",
    "            page_number += 1\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GoldenPages data\n",
    "goldenpages_rows = scrape_goldenpages()\n",
    "current_total = len(goldenpages_rows)\n",
    "print(\"\\nGoldenPages collected:\", current_total, \"rows\")\n",
    "\n",
    "# Then collect Yelp data if needed to reach the MAX_ROWS target\n",
    "if current_total < MAX_ROWS:\n",
    "    remaining = MAX_ROWS - current_total\n",
    "    print(\"Additional rows required:\", remaining)\n",
    "    yelp_rows = scrape_yelp(max_rows_remaining=remaining)\n",
    "else:\n",
    "    print(\"Reached MAX_ROWS from GoldenPages; skipping Yelp\")\n",
    "    yelp_rows = []\n",
    "\n",
    "# Combine both sources\n",
    "all_rows = goldenpages_rows + yelp_rows\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "print(\"\\nRows before duplicate removal:\", len(df))\n",
    "\n",
    "# Then remove duplicates using fields available in class\n",
    "df = df.drop_duplicates(subset=[\"name\", \"address\"], keep=\"first\")\n",
    "\n",
    "print(\"Rows after duplicate removal:\", len(df))\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"../data/dataProject.csv\", index=False)\n",
    "print(\"\\nDataset saved to dataProject.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b40de3",
   "metadata": {},
   "source": [
    "## Data Cleaning and Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d89e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1091 entries, 0 to 1090\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   source              1091 non-null   object \n",
      " 1   category_search     1071 non-null   object \n",
      " 2   name                1091 non-null   object \n",
      " 3   address             1071 non-null   object \n",
      " 4   phone               1066 non-null   object \n",
      " 5   category_from_page  1071 non-null   object \n",
      " 6   summary             397 non-null    object \n",
      " 7   region              20 non-null     object \n",
      " 8   rating_raw          20 non-null     float64\n",
      " 9   review_count_raw    20 non-null     object \n",
      " 10  location            20 non-null     object \n",
      " 11  price_range         18 non-null     object \n",
      " 12  categories          20 non-null     object \n",
      " 13  snippet             20 non-null     object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 119.5+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9f82d16b-3dcf-4c8f-8ff1-44a3cb4254da",
       "rows": [
        [
         "source",
         "0"
        ],
        [
         "category_search",
         "20"
        ],
        [
         "name",
         "0"
        ],
        [
         "address",
         "20"
        ],
        [
         "phone",
         "25"
        ],
        [
         "category_from_page",
         "20"
        ],
        [
         "summary",
         "694"
        ],
        [
         "region",
         "1071"
        ],
        [
         "rating_raw",
         "1071"
        ],
        [
         "review_count_raw",
         "1071"
        ],
        [
         "location",
         "1071"
        ],
        [
         "price_range",
         "1073"
        ],
        [
         "categories",
         "1071"
        ],
        [
         "snippet",
         "1071"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 14
       }
      },
      "text/plain": [
       "source                   0\n",
       "category_search         20\n",
       "name                     0\n",
       "address                 20\n",
       "phone                   25\n",
       "category_from_page      20\n",
       "summary                694\n",
       "region                1071\n",
       "rating_raw            1071\n",
       "review_count_raw      1071\n",
       "location              1071\n",
       "price_range           1073\n",
       "categories            1071\n",
       "snippet               1071\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataProject.csv\")\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228c388",
   "metadata": {},
   "source": [
    "### Data Quality Summary\n",
    "\n",
    "The combined dataset contains 1,091 rows and 14 columns. These rows come from two different sources (GoldenPages and Yelp), each providing different types of information. Because of this, the pattern of missing values is expected and consistent with the nature of each source.\n",
    "\n",
    "**GoldenPages rows**\n",
    "GoldenPages listings include:\n",
    "- business name\n",
    "- category used in the search\n",
    "- address\n",
    "- phone number\n",
    "- short business summary (when available)\n",
    "\n",
    "They **do not contain**:\n",
    "- ratings\n",
    "- review counts\n",
    "- price range\n",
    "- categories list\n",
    "- customer review snippets\n",
    "- region\n",
    "\n",
    "This explains why these columns show a large number of missing values (around ~1070 missing entries each).\n",
    "\n",
    "**Yelp rows**\n",
    "Yelp listings include:\n",
    "- name\n",
    "- rating\n",
    "- review count\n",
    "- location text\n",
    "- price range\n",
    "- category tags\n",
    "- customer review snippet\n",
    "- region label\n",
    "\n",
    "They **do not contain**:\n",
    "- phone number\n",
    "- GoldenPages summary\n",
    "- GoldenPages category info  \n",
    "\n",
    "This explains why `phone`, `summary`, `category_search`, and `category_from_page` have missing values for most rows.\n",
    "\n",
    "### Interpretation of Missing Values\n",
    "\n",
    "The missing values are *not* data errors — they are a structural consequence of merging two datasets with different schemas. Each row contains only the attributes provided by its source.\n",
    "\n",
    "- GoldenPages contributes **~1,071 rows**\n",
    "- Yelp contributes **~20 rows** after deduplication\n",
    "- Columns that belong only to one source appear as `NaN` for the other\n",
    "\n",
    "This is expected behaviour in a multi-source scraping project, and the dataset is suitable for exploration and basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Counting Bakeries by Region (YELP data only)\n",
    "df[df['source']==\"Yelp\"]['region'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Number of Bakeries per Region (Yelp)\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Ratings Distribution (also YELP data only)\n",
    "df['rating_raw'] = pd.to_numeric(df['rating_raw'], errors='coerce')\n",
    "\n",
    "df[df['source']==\"Yelp\"]['rating_raw'].plot(kind=\"hist\", bins=10)\n",
    "plt.title(\"Distribution of Bakery Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764b15b",
   "metadata": {},
   "source": [
    "### Business Insights\n",
    "- Dublin has the highest number of bakeries listed on Yelp, indicating strong competition and demand.\n",
    "- Rating distribution suggests that most bakeries in Ireland receive positive reviews.\n",
    "- GoldenPages data is more focused on basic business listings, whereas Yelp provides customer perception."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
